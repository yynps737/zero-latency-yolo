# 零延迟YOLO FPS云辅助系统模型文件

## 模型文件说明

此目录用于存放系统使用的YOLO检测模型文件。系统默认使用的模型为`yolo_nano_cs16.onnx`，这是一个为CS 1.6优化的轻量级检测模型。

### 模型详情

- **文件名**: yolo_nano_cs16.onnx
- **模型类型**: YOLO-nano (轻量级YOLO变体)
- **量化方式**: INT8量化
- **输入尺寸**: 416x416 像素, RGB, 3通道
- **输出格式**: 边界框, 置信度, 类别 (CS 1.6玩家模型)
- **支持类别**: 
  - CLASS_T (0): T方玩家
  - CLASS_CT (1): CT方玩家
  - CLASS_HEAD (2): 头部
  - CLASS_WEAPON (3): 武器

## 获取模型文件

由于模型文件较大，未包含在代码仓库中。您可以通过以下方式获取模型文件：

1. **下载预训练模型**:
   从我们的模型仓库下载预训练模型文件：
   ```
   curl -o yolo_nano_cs16.onnx https://your-model-repository-url/yolo_nano_cs16.onnx
   ```

2. **使用自己训练的模型**:
   如果您有自己训练的YOLO模型，可将其导出为ONNX格式并放置在此目录中。请确保模型输入输出与系统要求兼容。

3. **使用模型转换脚本**:
   如果您有YOLO权重文件(.weights)，可以使用如下命令将其转换为ONNX格式：
   ```
   python3 scripts/convert_model.py --weights path/to/your/model.weights --output models/yolo_nano_cs16.onnx --quantize int8
   ```

## 模型优化说明

为了在资源受限的2核4G服务器上实现高性能，我们对模型进行了以下优化：

1. **INT8量化**: 将浮点权重量化为8位整数，降低内存使用并提升推理速度
2. **剪枝优化**: 移除对检测任务贡献较小的卷积通道
3. **注意力机制优化**: 简化注意力模块，减少计算量
4. **骨干网络压缩**: 使用轻量级骨干网络，牺牲部分精度换取速度

## 自定义和替换模型

您可以使用自己的自定义模型替换默认模型。如需替换模型，请确保：

1. 新模型必须导出为ONNX格式
2. 输入尺寸必须为416x416或在服务器配置文件中相应调整
3. 输出格式需与系统预期一致，或修改yolo_engine.cpp中的后处理逻辑
4. 对于资源受限环境，推荐使用INT8量化模型

如果您使用不同名称的模型文件，需在`configs/server.json`中更新`model_path`设置。